{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Capstone Project\n",
    "## Project : Recognizing hardwritten digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we will discover the MNIST handwritten digit recognition problem and we will develop a deep learning model in Python using the Keras library that will be capable of achieving excellent results. Now some questions come to mind:\n",
    "- What is deep learning ? \n",
    "- Deep Learning is a subfield of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks.\n",
    "- Why we are using keras ?\n",
    "- Keras is one of the best neural network Frameworks currently around. Here are the main advantages I see for it:\n",
    "  - Simplicity (code itself is very good)\n",
    "  - Awesome community - there is good documentation, lots of community code, it is also the framework of choice for many Kagglers nowadays meaning you can get a constant stream of some really insightful data science competition grade code written in keras.\n",
    "  - Very active development.\n",
    "- What is MNIST handwritten digit recognition problem ?\n",
    "- The MNIST problem is a dataset developed by Yann LeCun, Corinna Cortes and Christopher Burges for evaluating machine learning models on the handwritten digit classification problem.The dataset was constructed from a number of scanned document dataset available from the National Institute of Standards and Technology (NIST). This is where the name for the dataset comes from, as the Modified NIST or MNIST dataset. Images of digits were taken from a variety of scanned documents, normalized in size and centered. This makes it an excellent dataset for evaluating models, allowing the developer to focus on the machine learning with very little data cleaning or preparation required. In this tutorial, we’ll give you a step by step walk-through of how to build a hand-written digit classifier using the MNIST dataset. For someone new to deep learning, this exercise is arguably the “Hello World” equivalent. It consists of 70,000 labeled 28x28 pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images. There are 10 classes (one for each of the 10 digits). The task at hand is to train a model using the 60,000 training images and subsequently test its classification accuracy on the 10,000 test images.\n",
    "\n",
    "### Let's Start\n",
    "\n",
    "Let's start by importing numpy and setting a seed for the computer's pseudorandom number generator. This allows us to reproduce the results from our script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import numpy\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(10)  # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "\n",
    "### Load image data from MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 60000 training images.\n",
      "There are 10000 testing images.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# The number of training and testing images present in the dataset\n",
    "print('There are %d training images.' % len(X_train))\n",
    "print('There are %d testing images.' % len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some samples from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD7CAYAAAB68m/qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6EREluCATkRkif8HxddPmCX4msUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fa2e9c0080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot some random images in default as well as gray scale format\n",
    "plt.subplot(231)\n",
    "plt.imshow(X_train[10])\n",
    "plt.subplot(232)\n",
    "plt.imshow(X_train[78])\n",
    "plt.subplot(233)\n",
    "plt.imshow(X_train[22])\n",
    "plt.subplot(234)\n",
    "plt.imshow(X_train[43], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(235)\n",
    "plt.imshow(X_train[15], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(236)\n",
    "plt.imshow(X_train[20], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to take a look at dimensions of the input images from the dataset so that we can decide whether it needs preprocessing or not before getting it as input to the neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training dataset:\n",
      "(60000, 28, 28)\n",
      "Shape of testing dataset:\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# printing shape of training dataset\n",
    "print ('Shape of training dataset:') \n",
    "print (X_train.shape)\n",
    "\n",
    "# printing shape of testing dataset\n",
    "print ('Shape of testing dataset:')\n",
    "print (X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preprocessing Data**\n",
    "\n",
    "### Reshaping Dimesions\n",
    "Now we will preprocess the inputs for keras. Our input is structured as a 3d array in the form of (60000, 28, 28), this means the height and depth of image is 28x28 and 60000 is number of images in the dataset. But input in keras requires 4d array. We must add a dimension for the depth of image. A full-color image with all 3 RGB channels will have a depth of 3, but MNIST images only have a depth of 1 and we can add this dimension easily using the **reshape()** function on the NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions after reshaping:\n",
      "(60000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Reshape input data \n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "\n",
    "# Printing the new dimensions\n",
    "print('Dimensions after reshaping:')\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the Pixel values\n",
    "Each of the pixels that represents an image stored inside a computer has a pixel value which describes how bright that pixel is, and/or what color it should be. In the simplest case of binary images, the pixel value is a 1-bit number indicating either foreground or background. For a grayscale images, the pixel value is a single number that represents the brightness of the pixel. The most common  pixel format is the byte image, where this number is stored as an 8-bit integer giving a range of possible values from 0 to 255. Typically zero is taken to be black, and 255 is taken to be white. Values in between make up the different shades of gray. It is almost always a good idea to perform some scaling of input values when using neural network models. Because the scale is well known and well behaved, we can very quickly normalize the pixel values to the range 0 and 1 by dividing each value by the maximum of 255 and this will normalize our data values to the range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert data type to float32 \n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalizing pixel values\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess class labels for Keras\n",
    "This is a multi-class classification problem. The output variable is an integer from 0 to 9. Currently our labels is an array containing 10 classes and we need the labels to be in 10 distinct classes. We can fix this easily by using one hot encoding. There is a built-in **np_utils.to_categorical()** helper function in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the library needed for one hot encoding\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# One Hot Encode labels\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define model architecture**\n",
    "\n",
    "The Keras layers module provides a high-level API that makes it easy to construct a neural network. It provides methods that facilitate the creation of dense (fully connected) layers and convolutional layers, adding activation functions, and applying dropout regularization. Here, we will learn how to use layers to build a convolutional neural network model to recognize the handwritten digits in the MNIST data set.\n",
    "### **About CNN**\n",
    "Convolutional neural networks (CNNs) are the current state-of-the-art model architecture for image classification tasks. CNNs apply a series of filters to the raw pixel data of an image to extract and learn higher-level features, which the model can then use for classification. Our aim of introducing more and more layers is increasing the depth of image and decreasing the height and depth of the image. More the depth, more patterns the model will learn within the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 26, 26)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 13, 13)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 11, 11)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 5, 5)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 5, 5)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 225,034\n",
      "Trainable params: 225,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries for defining model architecture\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "# Define model architecture\n",
    "model = Sequential()\n",
    " \n",
    "model.add(Convolution2D(filters = 32, kernel_size = 3, activation='relu', input_shape=(1,28,28)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Convolution2D(filters = 64, kernel_size = 3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 113s 2ms/step - loss: 0.4067 - acc: 0.8729\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 117s 2ms/step - loss: 0.1298 - acc: 0.9610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fa32952668>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# Fit model on training data\n",
    "model.fit(X_train, Y_train, batch_size=200, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-1-49f590d6db38>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-49f590d6db38>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    print(UpSampling2D(size=(2, 2), data_format=None)\u001b[0m\n\u001b[1;37m                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import UpSampling2D\n",
    "\n",
    "\n",
    "# Evaluate model on test data\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print ('The error rate and Accuracy on test set is as follows : ')\n",
    "print (score)\n",
    "print(UpSampling2D(size=(2, 2), data_format=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
